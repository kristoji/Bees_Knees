#!/usr/bin/env python3
"""
Answer extraction module for HIVE LLM using regex patterns.
Extracts board states and moves from LLM generated text.
"""

import re
import torch
import numpy as np
from typing import Optional, Tuple, Dict, List
import logging

logger = logging.getLogger(__name__)


class HiveLLMAnswerExtractor:
    """Extract board states and moves from LLM output using regex patterns"""
    
    def __init__(self, tokenizer, projection_module):
        """
        Initialize the answer extractor
        
        Args:
            tokenizer: Tokenizer used by the LLM
            projection_module: Module to decode embeddings back to game space
        """
        self.tokenizer = tokenizer
        self.projection_module = projection_module
        
        # Define regex patterns for extracting answers
        self.board_pattern = r"Board\s+(\d+):\s*([^\s]+)"
        self.move_pattern = r"Move\s+(\d+):\s*([^\s]+)"
        
        # Combined pattern for extracting both board and move in expected output format
        self.output_pattern = r"Board\s+(\d+):\s*([^\s]+)\s+Move\s+(\d+):\s*([^\s]+)"
        
        # Pattern for extracting just the embeddings/tokens
        self.embedding_pattern = r"<([^>]+)>"
        
        # Special tokens for embeddings
        self.board_token = "<BOARD>"
        self.move_token = "<MOVE>"
        self.chosen_move_token = "<CHOSEN_MOVE>"
        self.next_state_token = "<NEXT_STATE>"
        
    def extract_from_text(self, generated_text: str) -> Dict[str, any]:
        """
        Extract board state and move information from generated text
        
        Args:
            generated_text: Text generated by the LLM
            
        Returns:
            Dictionary containing extracted information
        """
        result = {
            'board_number': None,
            'move_number': None,
            'board_text': None,
            'move_text': None,
            'success': False,
            'error': None
        }
        
        try:
            # Try to match the complete output pattern first
            output_match = re.search(self.output_pattern, generated_text, re.IGNORECASE)
            
            if output_match:
                result['board_number'] = int(output_match.group(1))
                result['board_text'] = output_match.group(2)
                result['move_number'] = int(output_match.group(3))
                result['move_text'] = output_match.group(4)
                result['success'] = True
                
                logger.debug(f"Successfully extracted: Board {result['board_number']}: {result['board_text']}, Move {result['move_number']}: {result['move_text']}")
                
            else:
                # Try to extract board and move separately
                board_match = re.search(self.board_pattern, generated_text, re.IGNORECASE)
                move_match = re.search(self.move_pattern, generated_text, re.IGNORECASE)
                
                if board_match:
                    result['board_number'] = int(board_match.group(1))
                    result['board_text'] = board_match.group(2)
                
                if move_match:
                    result['move_number'] = int(move_match.group(1))
                    result['move_text'] = move_match.group(2)
                
                if board_match or move_match:
                    result['success'] = True
                    logger.debug(f"Partially extracted: Board: {result['board_text']}, Move: {result['move_text']}")
                else:
                    result['error'] = "No matching patterns found in generated text"
                    logger.warning(f"Failed to extract from text: {generated_text}")
                    
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"Error extracting from text: {e}")
            
        return result
    
    def extract_embeddings_from_hidden_states(self, 
                                            hidden_states: torch.Tensor,
                                            attention_mask: torch.Tensor,
                                            target_positions: Optional[List[int]] = None) -> Dict[str, torch.Tensor]:
        """
        Extract embeddings from LLM hidden states at specific positions
        
        Args:
            hidden_states: Hidden states from LLM [batch_size, seq_len, hidden_dim]
            attention_mask: Attention mask [batch_size, seq_len]
            target_positions: Specific positions to extract (if None, extracts last 2 positions)
            
        Returns:
            Dictionary containing extracted embeddings
        """
        result = {
            'board_embedding': None,
            'move_embedding': None,
            'success': False,
            'error': None
        }
        
        try:
            batch_size, seq_len, hidden_dim = hidden_states.shape
            
            if target_positions is None:
                # Extract last 2 valid positions (assuming board state comes before move)
                for batch_idx in range(batch_size):
                    # Find last valid positions
                    valid_positions = torch.where(attention_mask[batch_idx] == 1)[0]
                    
                    if len(valid_positions) >= 2:
                        # Last position is move, second-to-last is board state
                        board_pos = valid_positions[-2].item()
                        move_pos = valid_positions[-1].item()
                        
                        result['board_embedding'] = hidden_states[batch_idx, board_pos, :]
                        result['move_embedding'] = hidden_states[batch_idx, move_pos, :]
                        result['success'] = True
                        
                        logger.debug(f"Extracted embeddings at positions: board={board_pos}, move={move_pos}")
                        break
                    else:
                        result['error'] = f"Not enough valid positions in sequence (found {len(valid_positions)})"
            else:
                # Extract from specified positions
                if len(target_positions) >= 2:
                    board_pos, move_pos = target_positions[-2], target_positions[-1]
                    result['board_embedding'] = hidden_states[0, board_pos, :]
                    result['move_embedding'] = hidden_states[0, move_pos, :]
                    result['success'] = True
                else:
                    result['error'] = "Not enough target positions specified"
                    
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"Error extracting embeddings: {e}")
            
        return result
    
    def decode_embeddings_to_game_space(self, 
                                      board_embedding: torch.Tensor,
                                      move_embedding: torch.Tensor) -> Dict[str, np.ndarray]:
        """
        Decode LLM embeddings back to game space using projection module
        
        Args:
            board_embedding: Board embedding in LLM space [hidden_dim]
            move_embedding: Move embedding in LLM space [hidden_dim]
            
        Returns:
            Dictionary containing decoded embeddings in game space
        """
        result = {
            'board_game_embedding': None,
            'move_game_embedding': None,
            'success': False,
            'error': None
        }
        
        try:
            with torch.no_grad():
                # Decode board embedding back to GIN space
                if hasattr(self.projection_module, 'state_projection_inverse'):
                    board_game_emb = self.projection_module.state_projection_inverse(
                        board_embedding.unsqueeze(0)
                    ).squeeze(0)
                    result['board_game_embedding'] = board_game_emb.cpu().numpy()
                else:
                    logger.warning("No inverse state projection available")
                    result['board_game_embedding'] = board_embedding.cpu().numpy()
                
                # Decode move embedding back to move space
                if hasattr(self.projection_module, 'move_projection_inverse'):
                    move_game_emb = self.projection_module.move_projection_inverse(
                        move_embedding.unsqueeze(0)
                    ).squeeze(0)
                    result['move_game_embedding'] = move_game_emb.cpu().numpy()
                else:
                    logger.warning("No inverse move projection available")
                    result['move_game_embedding'] = move_embedding.cpu().numpy()
                
                result['success'] = True
                
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"Error decoding embeddings: {e}")
            
        return result
    
    def extract_complete_answer(self, 
                              generated_text: str,
                              hidden_states: torch.Tensor,
                              attention_mask: torch.Tensor,
                              decode_to_game_space: bool = True) -> Dict[str, any]:
        """
        Complete extraction pipeline combining text and embedding extraction
        
        Args:
            generated_text: Generated text from LLM
            hidden_states: Hidden states from LLM generation
            attention_mask: Attention mask
            decode_to_game_space: Whether to decode embeddings back to game space
            
        Returns:
            Complete extraction results
        """
        # Extract from text
        text_result = self.extract_from_text(generated_text)
        
        # Extract embeddings
        embedding_result = self.extract_embeddings_from_hidden_states(hidden_states, attention_mask)
        
        # Combine results
        result = {
            'text_extraction': text_result,
            'embedding_extraction': embedding_result,
            'game_space_embeddings': None,
            'overall_success': text_result['success'] and embedding_result['success']
        }
        
        # Decode to game space if requested and embeddings were extracted successfully
        if decode_to_game_space and embedding_result['success']:
            decode_result = self.decode_embeddings_to_game_space(
                embedding_result['board_embedding'],
                embedding_result['move_embedding']
            )
            result['game_space_embeddings'] = decode_result
        
        return result
    
    def validate_output_format(self, generated_text: str, expected_board_num: int, expected_move_num: int) -> bool:
        """
        Validate that the generated text follows the expected format
        
        Args:
            generated_text: Generated text to validate
            expected_board_num: Expected board number
            expected_move_num: Expected move number
            
        Returns:
            True if format is valid, False otherwise
        """
        try:
            extraction = self.extract_from_text(generated_text)
            
            if not extraction['success']:
                return False
            
            # Check if numbers match expectations
            if (extraction['board_number'] == expected_board_num and 
                extraction['move_number'] == expected_move_num):
                return True
            else:
                logger.warning(f"Number mismatch: expected Board {expected_board_num}, Move {expected_move_num}, "
                             f"got Board {extraction['board_number']}, Move {extraction['move_number']}")
                return False
                
        except Exception as e:
            logger.error(f"Error validating output format: {e}")
            return False
    
    def extract_special_tokens(self, generated_text: str) -> List[str]:
        """
        Extract special embedding tokens from generated text
        
        Args:
            generated_text: Generated text containing special tokens
            
        Returns:
            List of extracted special tokens
        """
        matches = re.findall(self.embedding_pattern, generated_text)
        return matches
    
    def format_expected_output(self, board_num: int, move_num: int) -> str:
        """
        Format the expected output string for training/validation
        
        Args:
            board_num: Board number
            move_num: Move number
            
        Returns:
            Formatted expected output string
        """
        return f"Board {board_num}: {self.next_state_token} Move {move_num}: {self.chosen_move_token}"


def test_answer_extractor():
    """Test function for the answer extractor"""
    
    # Mock tokenizer and projection module for testing
    class MockTokenizer:
        pass
    
    class MockProjectionModule:
        pass
    
    extractor = HiveLLMAnswerExtractor(MockTokenizer(), MockProjectionModule())
    
    # Test cases
    test_cases = [
        "Board 5: <NEXT_STATE> Move 5: <CHOSEN_MOVE>",
        "Board 10: next_board_state_embedding_token Move 10: chosen_move_embedding_token",
        "Some text before Board 3: token1 Move 3: token2 some text after",
        "Invalid format without proper structure",
        "Board 7: embedding1 but missing move part"
    ]
    
    print("Testing HiveLLMAnswerExtractor:")
    for i, test_text in enumerate(test_cases):
        print(f"\nTest case {i+1}: {test_text}")
        result = extractor.extract_from_text(test_text)
        print(f"Success: {result['success']}")
        if result['success']:
            print(f"Board {result['board_number']}: {result['board_text']}")
            print(f"Move {result['move_number']}: {result['move_text']}")
        else:
            print(f"Error: {result['error']}")
    
    # Test validation
    print(f"\nValidation test:")
    valid_text = "Board 5: <NEXT_STATE> Move 5: <CHOSEN_MOVE>"
    is_valid = extractor.validate_output_format(valid_text, 5, 5)
    print(f"Is valid format: {is_valid}")


if __name__ == "__main__":
    test_answer_extractor()
